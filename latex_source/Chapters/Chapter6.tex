% Chapter Template

\chapter{Conclusion} % Main chapter title

\label{Chapter6} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 6. \emph{Conclusion}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

In this thesis, we successfully trained various \textit{interaction-based} neural ranking models on the standard TREC newswire collection (Robust04) and reproduced results that had a similar retrieval effectiveness or a retrieval performance close to what was reported in the papers across all the evaluation metrics. The few differences that we observed in our experiments were mainly due to the lack of information regarding the exact splits of the data that were used for cross-validation, or the exact hyper-parameters used to tune the model along with the amount of data that was sampled (number of iterations or epochs for which the model was trained, number of query-document pairs sampled) or how the ranking pairs were sampled from the relevance judgements. The two neural ranking models--PACRR-DRMM and NPRF-DRMM for which the above data was reported or shared, helped us train models that had retrieval effectiveness exactly as reported in the paper. We also investigated the reproducibility of DSSM, a \textit{representation-based} neural ranking model which has been shown to perform well only on large-scale click-through proprietary datasets. We had  tried various approaches that included pre-training the triletter representations using the target corpus, or weak supervision using AOL queries to generate a large set of training data and a combination of both, but couldn't train an effective model which was also highlighted in a parallel study~\citep{nie_ictir_18}. In the future, we would like to investigate in detail as to why we were not able train an effective model on the Robust04 newswire collection, and if given a large public dataset (MS-MARCO) can we train an effective DSSM model. %even if it was shown previously that \textit{representation-based} approaches can be trained effectively with weak supervision~\citep{Dehghani_sigir17}. 
This thesis  covers an extensive survey of the existing neural ranking approaches in the literature as well (Chapter~\ref{Chapter2}).

In this thesis, we also investigated the applicability of existing \textit{model-introspective} interpretability approaches in ML, such as, DeepSHAP for explaining the decisions of complex neural ranking models--\textit{why} a particular document was considered relevant for the query. We suggested several reference input methods for DeepSHAP that take into account the unique semantics of document ranking and relevance in IR. Through quantitative experiments we found that it is indeed sensitive to the reference input. The distinct lack of overlap in most cases was surprising but in line with recent works on the lack of robustness in interpretability approaches~\citep{Ghorbani2017}. We also tried to evaluate which reference method is more accurate by comparing against a \textit{model-agnostic} approach LIME. Here we found that the reference input method selection is highly dependent on the neural ranking model at hand. We believe that this work exposes new problems when dealing with model introspective interpretability for NRMs. 

A worthwhile endeavor will be to investigate new approaches that explicitly take into account the discreteness of text and the model's preprocessing choices when generating explanations. It would also be interesting to consider how we can answer the following ranking questions to help the user understand the underlying neural ranking model better:

\begin{itemize}
    \item \textsf{What is the intent of the query encoded by the neural ranking model?} -- Could we simply aggregate the term weights returned by DeepSHAP from top-\textit{k} results and pick the top-\textit{n} words to indicate intent of the query.
    \item \textsf{Why a particular document $d_A$ is ranked higher than another document $d_B$ in the ranked list?} -- Could we construct a `reference' input from the distribution of terms in the lower ranked document $d_B$ that would help pick terms which are different or unique that explain the higher relevance of $d_A$.
\end{itemize}
